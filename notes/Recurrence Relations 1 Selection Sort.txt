Selection Sort finds the smallest element in the list and swaps it with the
element at the front of the list (if the smallest element is not already
there).  Then the whole process is repeated on the (unsorted) rest of the
list.

The iterative version of selection sort has the following running time:

def selection_sort(lst):
    n = len(lst)                             # Linear time c0 * n
    for i in range(n - 1):                   # Executes n - 1 times
        min_index = i                        # Constant time c1
        for j in range(i + 1, n):            # Executes n - i - 1 times
            if lst[j] < lst[min_index]:      # Constant time c2
                min_index = j                # Constant time c1
        if min_index != i:                   # Constant time c3
            swap(lst, i, min_index)          # Constant time c4

The outer loop ranges from 0 to n - 2 and the inner loop ranges from i + 1
to n - 1.  The total running time T(n) (where n is the length of the list)
is then:

c0 * n + (c1 + c3) * (n - 1) + c2 * (n-1 + n-2 + ... + 1)
≤ T(n) ≤
c0 * n + (c1 + c3 + c4) * (n - 1) + (c2 + c1) * (n-1 + n-2 + ... + 1)

(The best case is when the list is already sorted and so no swap is
required, the worst case is when a swap is required on every iteration of
the outer loop.)

Since n-1 + n-2 + ... + 1 = n(n - 1)/2, we get that the running time
T(n) ∈ Θ(n²).  So the iterative version of selection sort always has a
quadratic running time (which is not very good, which is why selection sort
is not very interesting as a sorting algorithm).


The recursive version of selection sort has the following running time
(where m = n - i represents the length of the part of the list from index i
to index n - 1 that we are currently processing):

def selection_sort_rec_helper(lst, i, n):    # T(n - i) = T(m)
    if i >= n - 1:                           # Constant time c5
        return                               # Constant time c6
    min_index = i                            # Constant time c1
    for j in range(i + 1, n):                # Executes n - i - 1 = m - 1 times
        if lst[j] < lst[min_index]:          # Constant time c2
            min_index = j                    # Constant time c1
    if min_index != i:                       # Constant time c3
        swap(lst, i, min_index)              # Constant time c4
    selection_sort_rec_helper(lst, i + 1, n) # T(n - (i + 1)) = T(m - 1)

So we have (not counting the len(lst) operation which is done in
selection_sort_rec):

c5 + c6
≤ T(m) ≤
c5 + (c1 + c3 + c4) + (c2 + c1) * (m - 1) + T(m - 1)

The best case only happens in the last step of the sorting algorithm when m
is 1 (or 0 if the input list is empty) so (introducing c7 = c5 + c6 to
simplify):

T(1) = c7

The rest of the time we have (introducing new constants c8 = c1 + c2 and c9
to simplify):

T(m) = T(m - 1) + c8 * (m - 1) + c9

This is a recurrence relation of the general form: T(n) = aT(n-b) + f(n)

- The constant a is the number of times the recursive function calls itself
  inside its own code.
- The term n-b means this is a decrease-and-conquer algorithm (a general
  strategy / category of algorithms where the size of the input decreases
  in fixed steps of size b).
- The constant b is the amount by which the input data is decreased in a
  recursive call.
- The function f(n) is the amount of work performed in the code of the
  function excluding the recursive calls.


Recurrence relation: T(m) = T(m-1) + c8(m-1) + c9 and T(1) = c7

Backward substitution:

1) Replace m with m-1 in the original formula above to get a formula for
T(m-1), then use that formula to replace T(m-1) in the previous equation:

T(m-1) = T(m-2) + c8(m-2) + c9
so T(m) = (T(m-2) + c8(m-2) + c9) + c8(m-1) + c9
        = T(m-2) + c8(m-2 + m-1) + 2 c9

2) Replace m with m-2 in the original formula above to get a formula for
T(m-2), then use that formula to replace T(m-2) in the previous equation:

T(m-2) = T(m-3) + c8(m-3) + c9
so T(m) = (T(m-3) + c8(m-3) + c9) + c8(m-1 + m-2) + 2 c9
        = T(m-3) + c8(m-3 + m-2 + m-1) + 3 c9

3) Repeat the same process i times to get the general form of the equation:

T(m) = T(m-i) + c8(m-i + ... + m-2 + m-1) + i * c9

The idea here is to repeatedly use substitution to transform the original
recurrence relation into a known sum.

4) What to do with the term T(m-i)?  We know that we have the initial
condition T(1) = c7, so we decide to choose i to be such that m - i = 1.
That way in the next step we will be able to replace T(m-i) with T(1).
Rewriting m - i = 1, we then get i = m - 1.

5) Plug the value of i from step 4 into the general equation from step 3.
The equation becomes:

T(m) = T(1) + c8(1 + ... + m-2 + m-1) + (m-1)c9

Then use the initial condition T(1) = c7 and simplify the sum:

T(m) = c7 + c8(1 + ... + m-2 + m-1) + (m-1)c9
     = c7 + c8 m(m-1)/2 + (m-1)c9

This is the final result for T(m), from which it is then easy to prove that
T(n) ∈ Θ(n²).

So both the iterative and the recursive version are Θ(n²).

(In fact in both case the constant factor for the n² term is
c8/2 = (c1 + c2)/2 so not only do both versions of the algorithm have the
same rate of growth, but the constant factor is the same as well.)
